{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOEcxLQlHRY0K1MZrEuOWyw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PabloParadaSouto/Automatica/blob/master/DDPG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28FIdsHPSsvq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "# Definir la red neuronal actor\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, max_action):\n",
        "        super(Actor, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, 400)\n",
        "        self.fc2 = nn.Linear(400, 300)\n",
        "        self.fc3 = nn.Linear(300, action_dim)\n",
        "        self.max_action = max_action\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = torch.tanh(self.fc3(x)) * self.max_action\n",
        "        return x\n",
        "\n",
        "# Definir la red neuronal crítica\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(Critic, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim + action_dim, 400)\n",
        "        self.fc2 = nn.Linear(400, 300)\n",
        "        self.fc3 = nn.Linear(300, 1)\n",
        "\n",
        "    def forward(self, x, u):\n",
        "        x = F.relu(self.fc1(torch.cat([x, u], 1)))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Definir el algoritmo DDPG\n",
        "class DDPG:\n",
        "    def __init__(self, state_dim, action_dim, max_action):\n",
        "        self.actor = Actor(state_dim, action_dim, max_action).cuda()\n",
        "        self.actor_target = Actor(state_dim, action_dim, max_action).cuda()\n",
        "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=1e-4)\n",
        "\n",
        "        self.critic = Critic(state_dim, action_dim).cuda()\n",
        "        self.critic_target = Critic(state_dim, action_dim).cuda()\n",
        "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=1e-3)\n",
        "\n",
        "        self.max_action = max_action\n",
        "\n",
        "    #Selecciona la accion y la devuelve aplanada (flatten())\n",
        "    def select_action(self, state):\n",
        "        state = torch.Tensor(state.reshape(1, -1)).cuda()\n",
        "        return self.actor(state).cpu().data.numpy().flatten()\n",
        "\n",
        "    #.cuda() sireve para usar targeta grafica y que el proceso sea mas rapido\n",
        "    def train(self, replay_buffer, batch_size=64, gamma=0.99, tau=0.005):\n",
        "        state, action, next_state, reward, not_done = replay_buffer.sample(batch_size)\n",
        "\n",
        "        state = torch.Tensor(state).cuda()\n",
        "        #selecciona la accion segun el estado actual\n",
        "        action = torch.Tensor(action).cuda()\n",
        "        next_state = torch.Tensor(next_state).cuda()\n",
        "        reward = torch.Tensor(reward).reshape((batch_size, 1)).cuda()\n",
        "        not_done = torch.Tensor(not_done).reshape((batch_size, 1)).cuda()\n",
        "\n",
        "        #selecciona la accion siguiente segun el estado siguiente\n",
        "        next_action = self.actor_target(next_state)\n",
        "        #obtiene el resultado de la red target del critic\n",
        "        target_Q = self.critic_target(next_state, next_action)\n",
        "        #ecuacion de bellman, el detach() es para que no acumule gradientes\n",
        "        target_Q = reward + (not_done * gamma * target_Q).detach()\n",
        "\n",
        "        #q valor predicho por mi red critic\n",
        "        current_Q = self.critic(state, action)\n",
        "\n",
        "        #error entre la red critic y la target\n",
        "        critic_loss = F.mse_loss(current_Q, target_Q)\n",
        "\n",
        "        #reinicia los gradientes, backpropagation y aplica el optimizador\n",
        "        self.critic_optimizer.zero_grad()\n",
        "        critic_loss.backward()\n",
        "        self.critic_optimizer.step()\n",
        "\n",
        "        #error de la red actor\n",
        "        actor_loss = -self.critic(state, self.actor(state)).mean()\n",
        "\n",
        "        #reinicia los gradientes, backpropagation y aplica el optimizador\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "\n",
        "        #obteine los parametros de la red critic y de la target\n",
        "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "            target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "\n",
        "        #obteine los parametros de la red actor y de la target\n",
        "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "            target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "\n",
        "# Configuración de la semilla aleatoria\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "\n",
        "# Definir el entorno\n",
        "env = gym.make('Pendulum-v0')\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "max_action = float(env.action_space.high[0])\n",
        "\n",
        "# Inicializar el agente DDPG\n",
        "agent = DDPG(state_dim, action_dim, max_action)\n",
        "\n",
        "# Configuración de hiperparámetros\n",
        "max_episodes = 1000\n",
        "max_steps = 500\n",
        "batch_size = 64\n",
        "\n",
        "# Entrenamiento\n",
        "for episode in range(max_episodes):\n",
        "    state = env.reset()\n",
        "    episode_reward = 0\n",
        "\n",
        "    for step in range(max_steps):\n",
        "        action = agent.select_action(state)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        agent.train(replay_buffer)\n",
        "        state = next_state\n",
        "        episode_reward += reward\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    print(\"Episode: {}, Reward: {}\".format(episode, episode_reward))\n"
      ]
    }
  ]
}