{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOI8MiDkGNBIfSgpZ7g9Dg4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PabloParadaSouto/Automatica/blob/master/ACTOR-CRITIC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mr6KzT7t0mq2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import gym"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir la red neuronal para el actor\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(Actor, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 64)\n",
        "        self.fc2 = nn.Linear(64, output_dim)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return self.softmax(x)\n",
        "\n",
        "# Definir la red neuronal para el cr√≠tico\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(Critic, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 64)\n",
        "        self.fc2 = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        return self.fc2(x)\n",
        "\n",
        "# Implementar el algoritmo ACTOR-Critic\n",
        "def actor_critic(env_name, max_episodes):\n",
        "    env = gym.make(env_name)\n",
        "    input_dim = env.observation_space.shape[0]\n",
        "    output_dim = env.action_space.n\n",
        "\n",
        "    actor = Actor(input_dim, output_dim)\n",
        "    critic = Critic(input_dim)\n",
        "    actor_optimizer = optim.Adam(actor.parameters(), lr=0.001)\n",
        "    critic_optimizer = optim.Adam(critic.parameters(), lr=0.001)\n",
        "\n",
        "    for episode in range(max_episodes):\n",
        "        state = env.reset()\n",
        "        rewards = []\n",
        "        log_probs = []\n",
        "\n",
        "        while True:\n",
        "            action_probs = actor(torch.FloatTensor(state))\n",
        "            action = torch.multinomial(action_probs, 1).item()\n",
        "            log_probs.append(torch.log(action_probs[action]))\n",
        "\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            rewards.append(reward)\n",
        "\n",
        "            if done:\n",
        "                Q_values = []\n",
        "                Q_value = 0\n",
        "                for r in rewards[::-1]:\n",
        "                    Q_value = r + 0.99 * Q_value\n",
        "                    Q_values.insert(0, Q_value)\n",
        "\n",
        "                Q_values = torch.tensor(Q_values)\n",
        "                log_probs = torch.stack(log_probs)\n",
        "                values = critic(torch.FloatTensor(state))\n",
        "                advantages = Q_values - values\n",
        "\n",
        "                actor_loss = -torch.mean(log_probs * advantages.detach())\n",
        "                critic_loss = torch.mean(advantages.pow(2))\n",
        "\n",
        "                actor_optimizer.zero_grad()\n",
        "                actor_loss.backward()\n",
        "                actor_optimizer.step()\n",
        "\n",
        "                critic_optimizer.zero_grad()\n",
        "                critic_loss.backward()\n",
        "                critic_optimizer.step()\n",
        "\n",
        "                break\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "# Ejecutar el algoritmo\n",
        "if __name__ == \"__main__\":\n",
        "    actor_critic('CartPole-v1', max_episodes=1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kY6e5UHD0sI5",
        "outputId": "a37281cf-cbc7-4aca-ef8b-dfed252100a8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uJY4okAJ0zWH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}